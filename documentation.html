---
layout: default
---
<section>
  <h1>
    <a name="create-a-corpus" class="anchor" href="#create-a-corpus"><span class="octicon octicon-link"></span></a>Create a Corpus</h1>
  <p>Begin by creating a new corpus. One of txtorg's strengths is its capacity to manage multiple corpora simultaneously. You can 
  create a corpus for one project, begin another, and later return to the original without reindexing, thus unifying the text management
    process into a single piece of software.</p>

  <p>To create a new corpus, click 'File' -> 'New Corpus'. You will be prompted to choose a directory in which to save the corpus, and then 
  to name it. You may put the corpus wherever you like. The corpus name may only contain alphanumeric characters.</p>
  <h1>
    <a name="import-documents" class="anchor" href="#create-a-corpus"><span class="octicon octicon-link"></span></a>Import Documents</h1>
  <p>Next, you must import documents. This section lays out the
  various formatting and import options and concludes with example
  scripts demonstrating the construction of files that can be used to 
  import documents into txtorg.</p>

  <h3>
    <a name="formatting" class="anchor" href="#formatting"><span class="octicon octicon-link"></span></a>Formatting Requirements</h3>
  <p>To import documents, click on the corpus to which you want to import documents (in the leftmost 'Corpus' frame). The corpus should be highlighted.
  Then, click 'Corpus' -> 'Import Documents'. You'll be shown a window labeled 'SELECT PREPROCESSING OPTIONS', at the bottom of which, you select
  the 'Corpus Format'. Note that the txtorg installation contains examples for all three import options, noted in the following paragraphs. The example
  is composed of excerpts from The Brothers Karamazov (now public domain) and books and chapters to which those excerpts belong (metadata).</p>

    <h4>Import an Entire Directory</h4>

  <p>The first import format option is 'Import an entire directory'. If you'd like to import all documents in a directory and any child directory
    located within it, select this option. Note that these documents must be .txt files, txtorg will skip all other file extensions but will import
    every .txt file located within the directory. Note this import option does not support metadata. An example of this import format can be found
    in the 'examples/brothersk/' directory contained in the zip file used to install txtorg.</p>


  <h4>Import from a CSV file (not including content)</h4>
  <p>The second import format option is 'Import from a CSV file (not including content)'. In this format, files are uploaded
  with a csv, where the first column is 'filepaths' pointing to all of the documents that are to be uploaded. The remaining 
  columns in the csv are metadata fields, such as author name or date. In the installation directory, '/examples/brothersk_without_content.csv'
  displays this import format. This file is also shown below. Note that txtorg cannot recognize relative directory paths (such as those 
  using a period to signify the current directory) on some operating systems, and thus to upload the example corpus with this CSV, you must 
  edit the paths accordingly.</p>

<pre><code>filepath,book,chapter
[full_path_to_directory]/examples/brothersk/1.txt,2,2
[full_path_to_directory]/examples/brothersk/2.txt,2,5
[full_path_to_directory]/examples/brothersk/3.txt,2,7
[full_path_to_directory]/examples/brothersk/4.txt,3,3
[full_path_to_directory]/examples/brothersk/5.txt,3,9
[full_path_to_directory]/examples/brothersk/6.txt,4,3
[full_path_to_directory]/examples/brothersk/7.txt,4,8
[full_path_to_directory]/examples/brothersk/8.txt,4,8
[full_path_to_directory]/examples/brothersk/9.txt,7,3</code></pre>

  <h4>Import from a CSV file (including content)</h4>
  <p>The third import format option is 'Import from a CSV file (including content)'. In this format, the documents are
  represented as a field in the csv, where the values are the actual text of the corpus, and the other columns again correspond
  to document metadata. In the installation directory, '/examples/brothersk_with_content.csv' displays an example of this import 
  format. This file is also shown below and can be uploaded without modification. Note that you'll be prompted to select the 
  field containing the content, so it need not necessarily be the first column in the CSV.</p>

  <pre><code>quote,book,chapter
The man who lies to himself and listens to his own lie comes to such a pass that he cannot distinguish the truth within him or around him and so loses all respect for himself and for others. And having no respect he ceases to love and in order to occupy and distract himself without love he gives way to passions and coarse pleasures and sinks to bestiality in his vices all from continual lying to other men and to himself,2,2
All this exile to hard labor and formerly with floggings does not reform anyone and above all does not even frighten almost any criminal and the number of crimes not only does not diminish but increases all the more. Surely you will admit that. And it turns out that society thus is not protected at all for although the harmful member is mechanically cut off and sent away far out of sight another criminal appears at once to take his place perhaps even two others. If anything protects society even in our time and even reforms the criminal himself and transforms him into a different person again it is Christ's law alone which manifests itself in the acknowledgement of one's own conscience,2,5
You're a Karamazov too! In your family sensuality is carried to the point of fever. So these three sensualists are now eyeing each other with knives in their boots. The thereof them are at loggerheads and maybe you're the fourth,2,7
I'm a Karamazov. . . . when I fall into the abyss I go straight into it head down and heels up and I'm even pleased that I'm falling in such a humiliating position and for me I find it beautiful. And so in that very shame I suddenly begin a hymn. Let me be cursed let me be base and vile but let me also kiss the hem of that garment in which my God is clothed; let me be following the devil at the same time but still I am also your son Lord and I love you and I feel a joy without which the world cannot stand and be,3,3
Viper will eat viper and it would serve them both right!,3,9
My friends ask gladness from God. Be glad as children as birds in the sky. And let man's sin not disturb you in your efforts do not fear that it will dampen your endeavor and keep it from being fulfilled do not say 'Sin is strong impiety is strong the bad environment is strong and we are lonely and powerless the bad environment will dampen us and keep our good endeavor from being fulfilled.' Flee from such despondency my children! There is only one salvation for you: take yourself up and make yourself responsible for all the sins of men. For indeed it is so my friend and the moment you make yourself sincerely responsible for everything and everyone you will see at once that it is really so that it is you who are guilty on behalf of all and for all. Whereas by shifting your own laziness and powerlessness onto others you will end by sharing in Satan's pride and murmuring against God,4,3
his whole heart blazed up and turned towards some kind of light and he wanted to live and live to go on and on along some path towards the new beckoning light and to hurry hurry right now at once!,4,8
Everything is permitted,4,8
Just know one thing Rakitka I may be wicked but still I gave an onion,7,3</code></pre>

  <h3>
    <a name="building-csv" class="anchor" href="#building-csv"><span class="octicon octicon-link"></span></a>Building an Import CSV</h3>
  
  <p>To import a corpus with metadata, you must construct a CSV like the
  two outlined in the previous section. To demonstrate how users might 
  make appropriate import CSVs, we provide examples in R and Python.
  For both examples, we scrape the Brothers Karamazov corpus from the
  web, then write it to the working directory in a format commonly used
  to denote metadata. Specifically, metadata is denoted in the filename.
  There are two metadata fields, the book and chapter in which the selected
  quotation can be found. The files are named [BOOK]_[CHAPTER].txt, where
  [BOOK] and [CHAPTER] denote the respective values for the particular
  document.</p>

  <p>You can of course construct the CSV by whatever method you find 
    most convenient, and you need only write one CSV. Our examples, in R
    and Python, write a CSV for importing with filenames and another
    for importing from a CSV that contains the actual documents.</p>

  <h4>Constructing an Import CSV with R</h4>

<pre><code>library('XML')

###################################
# SCRAPE CORPUS AND WRITE TO DISK #
###################################

base.url <- 'http://www.christopherlucas.org/data'

# Get links to docs
links.page <- paste(base.url, 'brothersk', sep = '/')
page <- htmlParse(links.page)
links <- getHTMLLinks(page)

# Create the directory structure, scrape the corpus, and write it to the disk
dir.create('scraped_docs')
setwd('./scraped_docs')
dir.create('4_8')
for(link in links){
    write(xpathSApply(
        htmlParse(paste(base.url, link, sep = '/')), '///p', xmlValue
        ),
          link)
}

###############################################
# FROM CORPUS ON DISK, MAKE TXTORG IMPORT CSV #
###############################################

files <- list.files(path = getwd(), recursive = TRUE)

# Import without content
m <- c()
for(file in files){
    book <- substr(file, 1, 1)
    chapter <- substr(file, 3, 3)
    m <- rbind(m, c(paste(getwd(), file, sep = '/'), book, chapter))
}
colnames(m) <- c('filename', 'book', 'chapter')
write.csv(m, 'txtorgCSV_without_content.csv', quote=FALSE, row.names = FALSE)

# Import with content in csv
m <- c()
for(file in files){
    book <- substr(file, 1, 1)
    chapter <- substr(file, 3, 3)
    content <- readLines(file)[1]
    m <- rbind(m, c(content, book, chapter))
}
colnames(m) <- c('content','book','chapter')
write.csv(m, 'txtorgCSV_with_content.csv', quote=FALSE, row.names = FALSE)</pre></code>

  <h4>Constructing an Import CSV with Python</h4>

<pre><code>import urllib2 
import lxml
from lxml import html
import os
import csv
import re

###################################
# SCRAPE CORPUS AND WRITE TO DISK #
###################################

base_url = 'http://www.christopherlucas.org/data/'

# Get links to docs
page = urllib2.urlopen(base_url + 'brothersk')
content = page.read()
page.close()

html = lxml.html.fromstring(content)
links = [i[2] for i in html.iterlinks()]

os.makedirs('corpus/4_8')
os.chdir('corpus')

for link in links:
    page = urllib2.urlopen(base_url + link)
    content = page.read()
    page.close()

    f = open(link, 'w')
    f.write(content)
    f.close()

###############################################
# FROM CORPUS ON DISK, MAKE TXTORG IMPORT CSV #
###############################################
    
# Import without content
f = open('txtorgCSV_without_content.csv','w')
names = ['filename', 'book', 'chapter']
dw = csv.DictWriter(f, names)
dw.writerow({k:k for k in names})
for root, dirnames, filenames in os.walk('.'):
    for filename in filenames:
        if not filename.endswith('.txt'):
            continue
        book = filename[0]
        chapter = filename[2]
        fname = os.getcwd() + filename
        dw.writerow({'filename':fname, 'book':book, 'chapter':chapter})
f.close()

# Import with content in csv
f = open('txtorgCSV_with_content.csv','w')
names = ['content', 'book', 'chapter']
dw = csv.DictWriter(f, names)
dw.writerow({k:k for k in names})
for root, dirnames, filenames in os.walk('.'):
    for filename in filenames:
        if not filename.endswith('.txt'):
            continue
        book = filename[0]
        chapter = filename[2]
        open_doc = open(root + '/' + filename, 'r')
        content = open_doc.read().rstrip()
        open_doc.close()        
        dw.writerow({'content':content, 'book':book, 'chapter':chapter})
f.close()</pre></code>

  <h3>
    <a name="encodings" class="anchor" href="#encodings"><span class="octicon octicon-link"></span></a>Encodings</h3>
  <p>txtorg supports a wide range of encodings through
  the <a href="https://pypi.python.org/pypi/chardet">Chardet</a>
  library in Python.  Those encodings can be seen in full in the
  encodings drop down menu on the import documents menu and are shown
  in the table below. In general, UTF-8 is preferable, in the event
  that you have a choice. If you select 'Automatically Detect
  Encodings', txtorg will guess the encodings of your corpus, with
  some error. Note that the corpus must be in one encoding, so txtorg
  will fail if you ask it to detect the encodings of a corpus
  containing multiple encodings. Because it isn't possible to detect
  encodings perfectly, it is better to designate the encoding if possible.</p>
  
  <h4>Supported Encodings</h4>
  <ul>
<li>ASCII, UTF-8, UTF-16 (2 variants), UTF-32 (4 variants)</li>
<li>Big5, GB2312, EUC-TW, HZ-GB-2312, ISO-2022-CN (Traditional and Simplified Chinese)</li>
<li>EUC-JP, SHIFT_JIS, ISO-2022-JP (Japanese)</li>
<li>EUC-KR, ISO-2022-KR (Korean)</li>
<li>KOI8-R, MacCyrillic, IBM855, IBM866, ISO-8859-5, windows-1251 (Cyrillic)</li>
<li>ISO-8859-2, windows-1250 (Hungarian)</li>
<li>ISO-8859-5, windows-1251 (Bulgarian)</li>
<li>windows-1252 (English)</li>
<li>ISO-8859-7, windows-1253 (Greek)</li>
<li>ISO-8859-8, windows-1255 (Visual and Logical Hebrew)</li>
<li>TIS-620 (Thai)</li>
</ul>

  <h3>
    <a name="dictionary" class="anchor" href="#dictionary"><span class="octicon octicon-link"></span></a>Dictionary Replacement</h3>
  <p>Often, corpora contain terms that you may wish to replace with another, either for cleaning purposes, or because you'd like to
  index a phrase. For example, an investigator may be interested in occurences of the term "Foreign Aid", rather than simply the terms
  "foreign" and "aid". Because txtorg only indexes unigrams, to include n-grams (n > 1) in the final TDM, an investigator should
  combine the n-grams of interest into a single token. For example, "foreign aid" becomes "foreignaid". The investigator can then 
  use this combined term to measure frequencies of the phrase "foreign aid".</p>

  <p>txtorg supports this sort of preprocessing by allowing the user to upload a two-column CSV, where the first column is the string
    that is to be replaced and the second column is the term with which it is to be replaced. 'examples/replace_dict.csv' displays a 
    simple example for use with the included Brothers Karamozov corpus. This example is displayed below, which will replace all occurences
    of the term 'karamozov' with 'smerdyakov' (another character name).</p>

  <pre><code>karamazov,smerdyakov</pre></code>

  <p>IMPORTANT: Note that the csv must be lower case, even if the terms that are to be replaced contain upper case letters.</p>

  <p>You'll note that there is a 'Simple replace' option in the window. If selected, all occurences of the terms in the 
    first column will be replaced with those in the second column. If not selected, txtorg will first tokenize the documents, then
    replace occurences of the tokens in the first column with those in the second. Thus, if you want to replace bi- or tri-grams 
    with a single term, select 'Simple replace'. However, this option is prone to false positives for simpler terms, so think
  carefully about the implications of both options.</p>

  <h3>
    <a name="custom" class="anchor" href="#custom"><span class="octicon octicon-link"></span></a>Custom Python Script</h3>
  <p>txtorg supports custom preprocessing scripts written in Python. You may wish to run your documents through a script 
  of your own to remove markup or to do some sort of unsupported preprocessing. If so, select 'Select Python script' and 
  select the Python script you'd like to run on your corpus. Txtorg will read the script, then will call a function 'custom()'
  on each document before adding it to the corpus. Thus, your script must contain a function 'custom()', which must take as 
  input a document (type str) and output a preprocessed document (also type str). All preprocessing must occur within the function
  'custom()', so you may wish to write your preprocessing functions as separate functions, then wrap them in a function named 'custom()'.
  However, we leave this up to the user, as the main point of this functionality is flexibility!</p>

  <p>If you write a custom preprocessing script, please send it to us, as we'd like to provide a repository of scripts
    created by users.</p>
  
  <h3>
    <a name="spellcheck" class="anchor" href="#spellcheck"><span class="octicon octicon-link"></span></a>Automated Spelling Correction</h3>
  <p>txtorg supports automated spellchecking in English. The spellchecker is quite rudimentary, so you ought not use it if your corpus 
  has a number of proper nouns or other complicated terms. However, if you'd like to run it, simply select the corresponding radio button
  and incorrectly spelled words will be replaced by their correct counterparts.</p>

  <h3>
    <a name="rebuild" class="anchor" href="#rebuild"><span class="octicon octicon-link"></span></a>Rebuild Index File</h3>
  <p>IMPORTANT: After importing documents, you must select the corpus,
    then click 'Corpus -> Rebuild Index File'. Also, note that in the
    examples above, both the second and the third import options yield
    the same corpus, while the first is identical except it has no
    metadata.</p>

  <h1>
    <a name="analyzer-selection" class="anchor" href="#analyzer-selection"><span class="octicon octicon-link"></span></a>Analyzer Selection (Specify Non-English Language)</h1>
  <p>At the core of txtorg is Apache Lucene (Cutting et al., 2013), a
    high-performance text search engine library. By drawing on the
    active open source Lucene community, txtorg is able to provide
    support for a diverse set of languages. txtorg currently includes
    support for Arabic, Bulgarian, Portuguese (separate tools for
    Brazil and Portugal), Catalan, Chinese, Japanese, Korean, Czech,
    Danish, German, Greek, English, Spanish, Basque, Persian,
    Finnish, French, Irish, Galician, Hindi, Hungarian, Armenian,
    Indonesian, Italian, Latvian, Dutch, Norwegian, Romanian, Russian,
    Swedish, Thai, and Turkish, among others.</p>

  <p>txtorg leverages the dedicated language-specific preprocessing
    utilities (stemming, segmentation, etc) that have been created by
    the open source Lucene community. Thus, if you select the Czech
    analyzer, for instance, txtorg will automatically process your
    text according to best practice for Czech language text. You can
    find more information about the
    analyzers <a href="http://lucene.apache.org/core/4_0_0/analyzers-common/overview-summary.html">here</a>.</p>

  <p>To select an analyzer, select the corpus by clicking on it, then click 'Corpus' -> 'Change Analyzer'. A window
    will appear, and in the left menu you may select an analyzer by clicking on it. For English text, we recommend the 
    EnglishAnalyzer. You can observe how each analyzer tokenizes the text by clicking text into the 'Sample' window, then 
    clicking 'Tokenize'. The tokens output by the selected analyzer will appear in the window below. Once you've decided
    on an analyzer, click 'OK'. txtorg will then reindex the documents given the new analyzer, so you must wait a moment (several moments
    for large corpora). A window will appear when the reindexing is complete, after which you may search for terms.</a>

  <h1>
    <a name="select-documents" class="anchor" href="#select-documents"><span class="octicon octicon-link"></span></a>Select Documents</h1>
  <p>To export a TDM, you must first select a TDM. In txtorg, this is done with Lucene queries, documented on this page and elsewhere on 
    the internet. txtorg supports all valid Lucene queries. Queries are input in the search box.</p>

  <h4>Select All Docs</h4>
  <p>To select all documents, search for 'all' (without quotes).</p>
        
<h4 class="boxed">Terms</h4>
<ul>
<li>A query is broken up into terms and operators. There are two types of terms: Single Terms and Phrases.</li>
<li>A Single Term is a single word such as "brothers" or "karamozov".</li>
<li>A Phrase is a group of words surrounded by double quotes such as "brothers karamozov".</li>
<li>Multiple terms can be combined together with Boolean operators to form a more complex query (see below).</li>
<li>Note: The analyzer used to create the index will be used on the terms and phrases in the query string.
        So it is important to choose an analyzer that will not interfere with the terms used in the query string.</li>
</ul>
        
<h4 class="boxed">Fields</h4>
<p>Lucene supports fielded (meta) data. When performing a search you
  can either specify a field, or use the default field (the content of
  the document).</p>
<ul>
<li>You can search any field by typing the field name followed by a colon ":" and then the term you are looking for. </li>
<li>As an example, let's search for documents in the second book containing the term 'karamazov'.</li>
<pre class="code">book:2 AND karamozov</pre>
</ul>
        
<h4 class="boxed">More Complicated Searches</h4>
<p>txtorg supports all valid Lucene queries, and Lucene queries can be incredibly complex. We point users to the official documentation <a href="http://lucene.apache.org/core/2_9_4/queryparsersyntax.html">here</a>
  for more information on the full range of search options.</a>

  <h1>
    <a name="export-tdm" class="anchor" href="#export-tdm"><span class="octicon octicon-link"></span></a>Export TDM</h1>
  <p>After you have selected the documents you want, you can export the documents as a document-term matrix or you can 
    export the full documents in their unprocessed form.</p>
  
  <h3>
    <a name="termfreq" class="anchor" href="#termfreq"><span class="octicon octicon-link"></span></a>Filter On Term Frequency</h3>
  <p>Infrequent and extremely frequent words often provide little
    additional information and can be stripped from the TDM. To do
    this, txtorg supports constraining the document-term matrix to
    terms within a user-specified frequency range. That is, users may
    bound the document-term matrix to terms that appear at least a
    certain number of times but no more than a specified number of
    times. To do so, in the rightmost window, simply enter the lower
    bound and the upper bound in the boxes for 'at least' and 'at
    most'. By default, these values indicate the least and most
    documents a term appears in.</p>

  <h3>
    <a name="format" class="anchor" href="#format"><span class="octicon octicon-link"></span></a>TDM Format</h3>
  <p>Next, you must click 'Export TDM'. The user will be asked to select one of three formats. 'Standard STM' is the 
    Blei et al format, also used in the STM package for R. If you intend to read the TDM into a topic modeling package, you 
    probably want this format. Delimited STM is the same, but the sparse matrix is delimited
    by commas, rather than spaces. And 'Flat CSV file' is simply a standard CSV, which you might want for simpler applications
    with just a few documents, or in rare occasions where the matrix is not sparse.</p> 

  <h3>
    <a name="exportfull" class="anchor" href="#exportfull"><span class="octicon octicon-link"></span></a>Export Full Documents</h3>
  <p>To export the full documents, simply click 'Export Files'. This can be especially helpful if you'd like to back out the full
  text associated with a row in the document-term matrix. For instance, you may want to use this with the plotQuote() function in 
  the stm package.</p>

  <h1>
    <a name="example" class="anchor" href="#example"><span class="octicon octicon-link"></span></a>Example</h1>

